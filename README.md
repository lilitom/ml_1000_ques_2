# 下载 
关注公众号"机器学习算法面试"，回复"资料"，即可领取整理好的资料<img src="https://upload-images.jianshu.io/upload_images/6363811-6d0044ec90ec58e0.png?imageMogr2/auto-orient/strip|imageView2/2/w/636/format/webp">
# 框架 
1.[机器学习理论](src/ch01_机器学习理论.md)
2.[模型选择与评估](src/ch02_模型选择与评估.md)
3.[线性模型与经典算法](src/ch03_线性模型与经典算法.md)
4.[树模型](src/ch04_树模型.md)
5.[非监督学习](src/ch05_非监督学习.md)
6.[集成学习](src/ch06_集成学习.md)
7.[深度学习理论](src/ch07_深度学习理论.md)
8.[常见深度学习模型](src/ch08_常见深度学习模型.md)
9.[其他](src/ch09_其他.md)

# 目录 
# 1.[机器学习理论](src/ch01_机器学习理论.md)
1.1 数学知识  1.1.1 机器学习中的距离和相似度度量方式有哪些？  1.1.2 马氏距离比欧式距离的异同点？  1.1.3 张量与矩阵的区别？  1.1.4 如何判断矩阵为正定？  1.1.5 距离的严格定义？  1.1.6 参考  1.2 学习理论  1.2.1 什么是表示学习？  1.2.2 什么是端到端学习？  1.2.3 机器学习的学习方式主要有哪些?  1.2.4 如何开展监督学习?  1.2.5 类别不均衡问题怎么做?  1.2.6 维度灾难是啥？怎么避免？  1.2.7 生成模型和判别模型的区别?  1.3 优化理论  1.3.1 什么是凸优化？  1.3.2 凸优化的优势是什么？  1.3.3 如何判断函数是否为凸的?  1.3.4 什么是鞍点？  1.3.5 解释什么是局部极小值，什么是全局极小值？  1.3.6 既然有全局最优，为什么还需要有局部最优呢？  1.3.7 机器学习有哪些优化方法？  1.3.8 梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？  1.3.9 解释一元函数极值判别法则是什么？  1.3.10 解释多元函数极值判别法则是什么？  1.3.11 什么是对偶问题？  1.3.12 随机梯度下降法、批量梯度下降法有哪些区别？  1.3.13 各种梯度下降法性能对比？  1.3.14 说一下梯度下降法缺点?  1.3.15 如何对梯度下降法进行调优?  1.3.16 随机梯度下降法、批量梯度下降法有哪些区别？  1.3.17 梯度下降法缺点  1.3.18 批量梯度下降和随机梯度下降法的缺点？  1.3.19 如何对梯度下降法进行调优?  1.3.20 各种梯度下降法性能比较  1.3.21 为什么归一化能加快梯度下降法求优化速度？  1.3.22 标准化和归一化有什么区别？  1.3.23 批量梯度下降和随机梯度下降法的缺点  1.3.24 极大似然估计和最小二乘法区别？  1.4 信息论  1.4.1 什么是信息增益？  1.4.2 熵是什么？  1.4.3 交叉熵表示的意义是什么？  1.4.4 KL散度是什么？  1.4.5 KL散度有哪些问题，该如何解决？  1.4.6 KL散度和交叉熵的区别？  1.4.7 参考  1.5 其他  1.5.1 分类问题标签长尾分布该怎么办？  1.5.2 当机器学习性能不是很好时，你会如何优化？  1.5.3 包含百万、上亿特征的数据在深度学习中怎么处理？  1.5.4 类别型数据你是如何处理的？比如游戏品类，地域，设备？  1.5.5 参考  # 2.[模型选择与评估](src/ch02_模型选择与评估.md)
2.1 损失函数类  2.1.1 代价函数，损失函数和目标函数的区别？  2.1.2 误差、偏差和方差的区别是啥？  2.1.3 常见的损失函数有哪些？  2.1.4 均方差损失函数和高斯假设的关系？  2.1.5 平均绝对误差损失函数和拉普拉斯假设的关系？  2.1.6 均方差损失函数与平均绝对误差损失函数区别?  2.1.7 mse对于异常样本的鲁棒性差的问题怎么解决？  2.1.8 介绍你了解到的熵的相关知识点？  2.1.9 交叉熵的设计思想是什么？  2.1.10 怎么衡量两个分布的差异？  2.1.11 Huber  2.1.12 为何使用Huber损失函数？  2.1.13 如何理解Hinger  2.1.14 交叉熵与最大似然估计的联系？  2.1.15 分类问题为何用交叉熵而不用MSE？  2.1.16 类别不均衡情况下使用什么损失函数？  2.1.17 参考  2.2 偏差与方差  2.2.1 什么是偏差和方差？  2.2.2 什么是噪声？  2.2.3 泛化误差、偏差和方差的关系？  2.2.4 偏差、方差与过拟合、欠拟合的关系？  2.2.5 偏差、方差与模型复杂度的关系?  2.2.6 请从偏差和方差的角度解释bagging和boosting的原理？  2.2.7 为什么说bagging是减少variance，而boosting是减少bias?  2.2.8 如何解决偏差、方差问题？  2.2.9 训练集上预测误差大，在测试集上预测误差小的情况？  2.2.10 参考  2.3 过拟合和欠拟合  2.3.1 什么是欠拟合？  2.3.2 什么是过拟合？  2.3.3 如何解决欠拟合?  2.3.4 过拟合原因有哪些？  2.3.5 如何解决过拟合?  2.3.6 为什么L1正则化会产生更稀疏？  2.3.7 为啥L1正则先验分布是Laplace分布，L2正则先验分布是Gaussian分布  2.3.8 Lasso回归的求解方法有哪些？  2.3.9 为什么L2正则化会产生更稠密解？  2.3.10 L1和L2的区别和联系？  2.3.11 为什么权重变小可以防止过拟合呢？  2.3.12 为什么增加样本可以减少过拟合？  2.3.13 参考  2.4 检验方法  2.4.1 比较检验方法有哪些？  2.4.2 什么是假设检验？  2.4.3 简述假设检验的一般步骤？  2.4.4 什么是置信区间？  2.4.5 为什么小样本用t检验？  2.4.6 各中检验方法的适用范围是什么？  2.4.7 相关性检验有那些标准？  2.4.8 参考  2.5 模型评估  2.5.1 什么是模型的泛化能力？  2.5.2 模型评估的方法主要有哪些？  2.5.3 Bootstrap原理以及抽样到的概率是啥？  2.5.4 自助法优缺点？  2.5.5 交叉验证的方法主要分为哪些？  2.5.6 k折交叉验证中k取值多少有什么关系？  2.5.7 训练集、验证集合测试集的作用？  2.5.8 划分数据集的比例选择方法?  2.5.9 调参的方法有哪些？  2.5.10 参考  2.6 性能度量  2.6.1 TP、FP、TN、FN具体指的是什么？  2.6.2 ROC曲线和PR曲线的区别？  2.6.3 如何综合precision和recall指标？  2.6.4 Precision和Recall的应用场景？  2.6.5 如何判断一个学习器的性能比另一个好？  2.6.6 ROC曲线中，高于和低于对角线表示意义?  2.6.7 多分类AUC怎么算？  2.6.8 ROC曲线和PR曲线的区别，适用场景，各自优缺点？  2.6.9 准确率Accuracy的局限性是什么？  2.6.10 AUC的物理意义是啥?  2.6.11 AUC为啥对正负样本比例不敏感？  2.6.12 为啥很多工程上的评价指标使用ROC或AUC  2.6.13 PR和ROC的区别？  2.6.14 为啥方差的计算公式分母为n-1?  2.6.15 为什么使用标准差？  2.6.16 回归问题的评价指标有哪些？  2.6.17 皮尔逊相关系数怎么算的？  2.6.18 参考  2.7 数据治理  2.7.1 机器学习中如何处理类别型特征？  2.7.2 机器学习中的异常值如何处理？  2.7.3 缺失值的处理方法有哪些？  2.7.4 如何进行连续特征离散化？  2.7.5 什么是特征工程？  2.7.6 特征工程的步骤有哪些？  2.7.7 特征离散化有什么好处？  2.7.8 特征归一化有哪些方法？  2.7.9 特征选择有哪些方法？  2.7.10 特征筛选如何获取高相似性特征？  2.7.11 计算特征之间的相关性方法有哪些？  2.7.12 如何检查数据中的噪声？  2.7.13 什么是组合特征？  2.7.14 如何处理高维特征？  2.8 不平衡问题  2.8.1 如何处理类别不均衡问题？  2.8.2 分类问题中如何解决正负样本比较大的情况？  2.8.3 采样后如何计算指标？  2.8.4 如果把不平衡的训练集采样到平衡，计算的AUC和Precision会右什么变化？  2.8.5 class_weight的思想是什么？  2.8.6 讲讲smote算法的原理?  2.8.7 smote的缺点以及为啥在业界用的不多？  2.8.8 过采样和生成样本的区别？  2.8.9 参考  # 3.[线性模型与经典算法](src/ch03_线性模型与经典算法.md)
3.1 PLA(感知机)  3.1.1 简单介绍下感知机算法？  3.1.2 单层感知机可以实现异或运算吗？  3.1.3 多层感知机可以解决异或问题吗？  3.1.4 感知机损失函数是什么？  3.1.5 感知机损失函数为什么不考虑W的二范数？  3.1.6 感知机优化算法是怎么做的？  3.1.7 感知机算法的解释唯一的吗？  3.1.8 感知机算法和SVM的区别？  3.1.9 参考  3.2 LR（线性回归）  3.2.1 简单介绍下线性回归？  3.2.2 线性回归的5大假设是什么？  3.2.3 线性回归要求因变量符合正态分布？  3.2.4 线性回归为啥做分类不好？  3.2.5 线性回归的损失函数是什么？  3.2.6 线性回归的求解方法有哪些？  3.2.7 线性回归在业界用的不多的原因有哪些？  3.2.8 为什么进行线性回归前需要对特征进行离散化处理？  3.2.9 线性回归时如果数据量太大导致无法一次读进内存如何解决？  3.2.10 线性回归中的R方是什么意思？  3.2.11 解释下R方为0是什么意思？  3.2.12 相关系数和R方的关系？  3.2.13 线性回归中的多重共线性是什么意思？  3.2.14 多重共线性的危害有哪些？  3.2.15 多重共线性是如何影响算法结果的？  3.2.16 共线性变量的处理有哪些方法？  3.2.17 线性回归优缺点？  3.2.18 请简单说下Lasso和Ridge的区别？  3.2.19 Ridge回归和Lasso回归的使用场景  3.2.20 参考  3.3 LR（逻辑回归）  3.3.1 简单介绍下LR算法？  3.3.2 LR是如何做分类的？  3.3.3 LR的损失函数怎么来的？  3.3.4 LR如何解决地维不可分？  3.3.5 LR的优缺点是什么？  3.3.6 LR在训练模型中出现了强相关特征怎么办？  3.3.7 为什么在进入LR模型前要将强相关特征去除？  3.3.8 逻辑回归与朴素贝叶斯有什么区别?  3.3.9 LR与NB有什么区别？  3.3.10 线性回归和LR的区别?  3.3.11 为什么LR的输出值可以作为概率？  3.3.12 LR和最大熵模型之间的关系到底是什么？  3.3.13 LR的并行化计算方法？  3.3.14 为什么LR适合稀疏矩阵？  3.3.15 LR为什么选择0.5作为分类的阈值？  3.3.16 LR都有哪些正则化？  3.3.17 LR能否用于非线性分类？  3.3.18 LR如何并行化？  3.3.19 SVM和LR区别？  3.3.20 为什么LR模型损失数使用交叉熵不用MSE？  3.3.21 为什么做LR之前要做归一化？  3.3.22 LR损失函数中为啥要加1/N  3.3.23 LR使用梯度下降法的时候的停止条件是什么？  3.3.24 LR是线性模型还是非线性模型？  3.3.25 请从多个角度解释下LR？  3.3.26 为什么LR要用极大似然法来进行参数估计？  3.3.27 参考  3.4 KNN  3.4.1 简单介绍下KNN？  3.4.2 KNN的实现方式有哪些？  3.4.3 KNN的决策边界是怎样的？  3.4.4 KD树与一维二叉查找树之间的区别?  3.4.5 KD树的构建过程是怎样的？  3.4.6 高维情况下KD树查找性能如何优化？  3.4.7 KNN数据需要归一化吗？  3.4.8 KNN的K设置的过大会有什么问题?  3.4.9 KD树建立过程中切分维度的顺序是否可以优化？  3.4.10 KNN为什么使用欧氏距离？  3.4.11 KNN中K是怎么选的？  3.4.12 KNN的优缺点有哪些？  3.4.13 如何进行分组计算来解决KNN计算量过大的问题？  3.4.14 KNN对不平衡样本的预测有哪些问题？  3.4.15 KNN分类和Kmeans的区别？  3.4.16 参考  3.5 SVM  3.5.1 请简单介绍下SVM？  3.5.2 为什么SVM要引入核函数？  3.5.3 SVM  3.5.4 SVM中的核函数有哪些？  3.5.5 SVM核函数之间的区别?  3.5.6 不同数据量和特征的情况下怎么选择核函数？  3.5.7 SVM中的函数间隔和几何间隔是什么？  3.5.8 SVM为什么引入对偶问题？  3.5.9 SVM中系数求解是怎么做的？  3.5.10 讲一下SVM中松弛变量和惩罚系数？  3.5.11 SVM在大数据情况下怎么办？  3.5.12 SVM为啥不加正则？  3.5.13 SVM和FM的区别？  3.5.14 说说为什么svm中的某核能映射到无穷维  3.5.15 SVM如何多分类？  3.5.16 为什么SVM对缺失数据敏感？  3.5.17 SVM的优缺点是什么？  3.5.18 说一下一下SVR的原理？  3.5.19 参考  3.6 NB  3.6.1 朴素贝叶斯算法优缺点？  3.6.2 什么是贝叶斯决策论？  3.6.3 贝叶斯公式是啥？  3.6.4 朴素怎么理解？  3.6.5 贝叶斯学派和频率学派的区别？  3.6.6 什么是拉普拉斯平滑？  3.6.7 朴素的缺点为什么有较好的表现效果？  3.6.8 朴素贝叶斯中有没有超参数可以调？  3.6.9 朴素贝叶斯中有多少种模型？  3.6.10 朴素贝叶斯有哪些应用吗？  3.6.11 朴素贝叶斯对异常值敏不敏感？  3.6.12 朴素贝叶斯对缺失值敏不敏感？  3.6.13 朴素贝叶斯是高方差还是低方差模型？  3.6.14 朴素贝叶斯为什么适合增量计算？  3.6.15 朴素贝叶斯与  3.6.16 高度相关的特征对朴素贝叶斯有什么影响？  3.6.17 参考  3.7 LDA(线性判别分析)  3.7.1 请简单介绍下LDA？  3.7.2 LDA和PCA的联系和区别？  3.7.3 LDA的优缺点？  3.7.4 LDA算法步骤简单说一下？  3.7.5 协方差为什么可以反映类内方差？  3.7.6 特征的辨识信息不是均值，LDA还可以用吗？  3.7.7 解释一下LDA的目标函数？  3.7.8 LDA需要对数据归一化吗？  3.7.9 参考  3.8 FM  3.8.1 简单介绍下FM？  3.8.2 为什么使用FM？  3.8.3 FM的公式是什么样的？  3.8.4 FM公式是如何化简的？  3.8.5 FM为什么要引入隐向量？  3.8.6 FM如何做优化的？  3.8.7 FM和SVM的区别？  3.8.8 FM和FFM的区别？  3.8.9 FM和LR的区别？  3.8.10 FFM中的F是什么意思？  3.8.11 FFM现实使用中存在哪些问题？  3.8.12 FM算法的优缺点是什么？  3.8.13 FM如何用于多路召回？  3.8.14 参考  # 4.[树模型](src/ch04_树模型.md)
4.1 基础树  4.1.1 为什么C4.5能处理连续特征而ID3不行？  4.1.2 剪枝的策略是啥?  4.1.3 树模型one_hot有哪些问题？  4.1.4 如何解决树模型中one_hot的问题?  4.1.5 为啥决策树后剪枝比预剪枝要好？  4.1.6 决策树中有哪些剪枝算法  4.2 提升树  4.2.1 为什么GBDT的树深度较RF通常都比较浅？  4.2.2 GBDT为什么使用cart回归树而不是使用分类树?  4.2.3 GBDT哪些部分可以并行？  4.2.4 GBDT与RF的区别？  4.2.5 简单介绍一下XGBoost  4.2.6 树模型怎么查看特征重要性  4.2.7 随机森林需要交叉验证吗？  4.2.8 XGBoost与GBDT的联系和区别有哪些？  4.2.9 为什么XGBoost泰勒二阶展开后效果就比较好呢？  4.2.10 XGBoost对缺失值是怎么处理的？  4.2.11 XGBoost为什么快  4.2.12 XGBoost防止过拟合的方法  4.2.13 XGBoost为什么若模型决策树的叶子节点值越大，越容易过拟合呢？  4.2.14 XGBoost为什么可以并行训练？  4.2.15 XGBoost中叶子结点的权重如何计算出来  4.2.16 XGBoost中的一棵树的停止生长条件  4.2.17 Xboost中的min_child_weight是什么意思  4.2.18 Xgboost中的gamma是什么意思  4.2.19 Xgboost中的参数  4.2.20 RF和GBDT的区别  4.2.21 xgboost本质上是树模型，能进行线性回归拟合么  4.2.22 Xgboos是如何调参的  4.2.23 为什么xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？  4.2.24 为什么常规的gbdt和xgboost不适用于类别特别多的特征?  4.2.25 怎么处理类别特征在树模型下？  4.2.26 GBDT的预测结果有负数，为啥？  4.2.27 参考  # 5.[非监督学习](src/ch05_非监督学习.md)
5.1 聚类算法  5.1.1 LDA和PCA区别？  5.1.2 解释下你知道的聚类算法以及他们之间的区别？  5.1.3 介绍下层次聚类算法?  5.1.4 LDA和PCA区别？  5.1.5 降维的必要性和目的是什么？  5.1.6 PCA和SVD的联系和区别？  5.1.7 除了PCA你还知道哪些降维方法  5.1.8 DBSCAN中的参数如何确定？  # 6.[集成学习](src/ch06_集成学习.md)
6.1 bagging  6.1.1 bagging和boosting区别  6.1.2 为啥adboost不容易过拟合？  6.1.3 为什么随机森林的泛化能力较强？  6.1.4 解释下stacking技术？  6.1.5 为什么bagging减少方差  6.1.6 什么场景下采用bagging集成方法  6.1.7 bagging和dropout区别  6.1.8 bagging和boosting的区别  6.1.9 为什么说bagging是减少variance，而boosting是减少bias?  6.1.10 请从偏差和方差的角度解释bagging和boosting的原理  6.1.11 详细说明下决策数如何计算特征重要性的？  6.1.12 softmax的这个小细节问题吗?  6.1.13 adaboost为什么不容易过拟合？  6.1.14 Random  6.1.15 组合弱学习器的算法？  # 7.[深度学习理论](src/ch07_深度学习理论.md)
# 8.[常见深度学习模型](src/ch08_常见深度学习模型.md)
# 9.[其他](src/ch09_其他.md)
