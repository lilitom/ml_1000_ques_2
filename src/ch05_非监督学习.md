[Toc]
  
# 5 非监督学习

  
## 5.1 聚类算法

  
### 5.1.1 LDA和PCA区别？

  相同点  
  (1) 两者的作用是用来降维的  
  (2) 两者都假设符合高斯分布  
    不同点  
  (1) LDA是有监督的降维方法，PCA是无监督的。  
  (2) LDA降维最多降到类别数K-1的维数，PCA没有这个限制。  
  (3) LDA更依赖均值，如果样本信息更依赖方差的话，效果将没有PCA好。  
  (4) LDA可能会过拟合数据。
    
### 5.1.2 解释下你知道的聚类算法以及他们之间的区别？

  各个聚类算法的对比如下：
  ![img](https://img-blog.csdnimg.cn/be63d6ad2f5f4406b9fe884f0be62f66.png)
    
### 5.1.3 介绍下层次聚类算法?

  根据层次分解的顺序是自底向上的还是自上向下的，层次聚类算法分为凝聚的层次聚类算法和分裂的层次聚类算法。　凝聚型层次聚类的策略是先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有对象都在一个簇中，或者某个终结条件被满足。绝大多数层次聚类属于凝聚型层次聚类，它们只是在簇间相似度的定义上有所不同。
    算法流程主要如下所示：  
  (1) 将每个对象看作一类，计算两两之间的最小距离；  
  (2) 将距离最小的两个类合并成一个新类；  
  (3) 重新计算新类与所有类之间的距离；  
  (4) 重复(2)、(3)，直到所有类最后合并成一类。 
    
### 5.1.4 LDA和PCA区别？

    1、相同点
    （1）两者的作用是用来降维的
    （2）两者都假设符合高斯分布
    2、不同点
    （1）LDA是有监督的降维方法，PCA是无监督的。
    （2）LDA降维最多降到类别数K-1的维数，PCA没有这个限制。
    （3）LDA更依赖均值，如果样本信息更依赖方差的话，效果将没有PCA好。
    （4）LDA可能会过拟合数据。
    ：https://blog.csdn.net/Chenzhi_2016/article/details/79451201
    
### 5.1.5 降维的必要性和目的是什么？

  降维的必要性：
    多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
  高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。
  过多的变量，对查找规律造成冗余麻烦。
  仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。
    降维的目的：
    减少预测变量的个数。
  确保这些变量是相互独立的。
  提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。
  数据在低维下更容易处理、更容易使用。
  去除数据噪声。
  降低算法运算开销。
    
### 5.1.6 PCA和SVD的联系和区别？

  1. 两者都是矩阵分解的技术，一个直接分解SVD，一个是对协方差矩阵操作后分解PCA
  2. 奇异值和特征向量存在关系，即有
  ```math
  {\lambda _i} = {s_i}^2/(n - 1)
  ```
  3. SVD可以获取另一个方向上的主成分，而PCA只能获得单个方向上的主成分，PCA只与SVD的右奇异向量的压缩效果相同
  4. 通过SVD可以得到PCA相同的结果，但是SVD通常比直接使用PCA更稳定。因为在PCA求协方差时很可能会丢失一些精度。例如Lauchli矩阵
                
### 5.1.7 除了PCA你还知道哪些降维方法

  当然PCA是众所周知的降维方法，SVD也是一种，除此之外，还有如LDA、LLE以及LE。
  - PCA  
  PCA也就是主成份分析，Principal Component Analysis(PCA)是现如今最流行的无监督线性降维方法之一了，其主要思想是数据经过某种投影，或者说乘以一个矩阵之后，得到的新的矩阵在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。PCA的目标主要如下：
  ```math
  \mathop {\max }\limits_w \frac{1}{m}\sum\limits_{i = 1}^m {{{({w^T}({x_i} - \bar x))}^2}}
  ```
  PCA追求的是在降维之后能够最大化保持数据的内在信息，并通过衡量在投影方向上的数据方差的大小来衡量该方向的重要性。但是这样投影以后对数据 的区分作用并不大，反而可能使得数据点揉杂在一起无法区分。这也是PCA存在的最大一个问题，这导致使用PCA在很多情况下的分类效果并不好。具体可以看下图所示，若使用PCA将数据点投影至一维空间上时，PCA会选择2轴，这使得原本很容易区分的两簇点被揉杂在一起变得无法区分；而这时若选择1轴将会得 到很好的区分结果。而下面所说的LDA就将数据映射到轴1上的。  
  ![image](https://img-blog.csdnimg.cn/ab9392f7a7dc4931b59295671a2a9cbb.png)  
  (2) LDA  
  Linear Discriminant Analysis(也有叫做Fisher Linear Discriminant)是一种有监督的（supervised）线性降维算法。与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分，如上图投影导轴1上，这里的公式推导就不说明。  
  (3) LLE  
  上面说到了线性降维方法，当然还有非线性降维方法，这里介绍下LLE，也就是局部线性嵌入，它能够使降维后的数据较好地保持原有流形结构 。LLE可以说是流形学习方法最经典的工作之一。很多后续的流形学习、降维方法都与LLE有密切联系。下图给了一个典型的例子，看到降维后数据还保持了流形的结构。
  
![image](https://www.icode9.com/i/ll/?i=20200316212600332.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODA1Mzg4Nw==,size_1,color_FFFFFF,t_70#pic_center)   

  (4) LE  
  Laplacian Eigenmaps 是用局部的角度去构建数据之间的关系。具体来讲，拉普拉斯特征映射是一种基于图的降维算法，它希望相互间有关系的点（在图中相连的点）在降维后的空间中尽可能的靠近，从而在降维后仍能保持原有的数据结构。 如果两个数据实例i和j很相似，那么i和j在降维后目标子空间中应该尽量接近。Laplacian Eigenmaps可以反映出数据内在的流形结构。 
  拉普拉斯特征映射通过构建邻接矩阵为W的图来重构数据流形的局部结构特征。其主要思想是，如果两个数据实例i和j很相似，那么i和j在降维后目标子空间中应该尽量接近。
  ![image](https://img-blog.csdnimg.cn/20191215213534273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwMTM2NA==,size_16,color_FFFFFF,t_70)
    
### 5.1.8 DBSCAN中的参数如何确定？

  DBSCAN也是一种较为常用的算法，DBSCAN中重要的参数是Eps和MinPts，那么这两个参数该如何确定呢？
    (1)  Eps的值可以使用绘制k-距离曲线(k-distance graph)方法得当，在k-距离曲线图明显拐点位置为对应较好的参数。若参数设置过小，大部分数据不能聚类；若参数设置过大，多个簇和大部分对象会归并到同一个簇中。
    K-距离：K距离的定义在DBSCAN算法原文中给出了详细解说，给定K邻域参数k,对于数据中的每个点，计算对应的第k个最近邻域距离，并将数据集所有点对应的最近邻域距离按照降序方式排序，称这幅图为排序的k距离图，选择该图中第一个谷值点位置对应的k距离值设定为Eps。一般将k值设为4。
    (2)  MinPts的选取有一个指导性的原则（a rule of thumb），MinPts≥dim+1,其中dim表示待聚类数据的维度。MinPts设置为1是不合理的，因为设置为1，则每个独立点都是一个簇，MinPts≤2时，与层次距离最近邻域结果相同，因此，MinPts必须选择大于等于3的值。若该值选取过小，则稀疏簇中结果由于密度小于MinPts，从而被认为是边界点儿不被用于在类的进一步扩展；若该值过大，则密度较大的两个邻近簇可能被合并为同一簇。因此，该值是否设置适当会对聚类结果造成较大影响。  